<!--

Hey, you!
Are you reading this in the generated README.md? Then you're in the wrong place!

The template and data used to generate the README is in `README.md.in` and `data.json`, respectively.

The README.md is generated using the following command: python3 generate_readme.py

-->

# üõ†Ô∏è Are Copilots Local Yet?

Current trends and state of the art for using open & local LLM models as copilots to complete code, generate projects, act as shell assistants, automatically fix bugs, and more.

üìù *Help keep this list relevant and up-to-date by [making edits][edit]!*

[edit]: https://github.com/ErikBjare/are-copilots-local-yet/edit/master/data/data.yaml

## Table of Contents

- [Summary](#-summary)
- [Background](#-background)
- [Editor Extensions](#-editor-extensions)
- [Tools](#-tools)
- [Chat Interfaces](#-chat-interfaces)
- [Models](#-models)
- [Datasets](#-datasets)
- [Misc Tools](#-misc-tools)
- [Suggested Setup](#-suggested-setup)
- [History](#-history)
- [Stats](#-stats)

## üìã Summary

Local Copilots are now fully functional, although with output quality still not on par with those offered by cloud-based services like GitHub Copilot.

This document is a curated list of local Copilots, shell assistants, and related projects. It is intended to be a resource for those interested in a survey of the existing tools, and to help developers discover the state of the art for projects like these.

## üìö Background

In 2021, GitHub released Copilot which quickly became popular among devs. Since then, with the flurry of AI developments around LLMs, local models that can run on consumer machines have become available, and it has seemed only a matter of time before Copilot will go local.

Many perceived limitations of GitHub's Copilot are related to its closed and cloud-hosted nature.

As an alternative, local Copilots enable:

- üåê Offline & private use
- ‚ö° Improved responsiveness
- üìö Better project/context awareness
- üéØ The ability to run models specialized for a particular language/task
- üîí [Constraining the LLM output](https://twitter.com/ErikBjare/status/1656731582001020928) to fit a particular format/syntax.

## üß© Editor Extensions

Editor extensions used to complete code using LLMs:

| Name          | Editor   | :star:  | Released | Notes     |
| ------------- | -------- | ------- | -------- | --------- |
{% for item in extensions %}| [{{ item.name }}{% if item.new %}NEW!{% endif %}]({{ item.link }}) | {{ item.editor | join(", ") }} | {{ item.stars }} | {{ item.released or '' }} | {{ item.notes }} |
{% endfor %}

## üõ†Ô∏è Tools

Tools that try to generate projects/features from specification:

| Name           | :star:  | Released  | Notes |
| -------------- | ------- | --------- | ----- |
{% for item in tools %}| [{{ item.name }}]({{ item.link }}) | {{ item.stars }} | {{ item.released }} | {{ item.notes }} |
{% endfor %}

## üó®Ô∏è Chat Interfaces

Chat interfaces with shell/REPL/notebook access.
Similar to/inspired by ChatGPT's "Advanced Data Analysis" feature (previously "Code Interpreter").

| Name           | :star:  | Notes     |
| -------------- | ------- | --------- |
{% for item in chat %}| [{{ item.name }}]({{ item.link }}) | {{ item.stars }} | {{ item.notes }} |
{% endfor %}

## ü§ñ Models

Models relevant for local Copilot-use. Ordered by most recent first.

| Name                            | Size       | Languages   | :star:  | Released   | Notes   |
| ------------------------------- | ---------- | ----------- | ------- | ---------- | ------- |
{% for item in models %}| [{{ item.name }}]({{ item.link }}) | {{ item.size }} | {{ item.languages }} | {{ item.stars }} | {{ item.released }} | {{ item.notes }} |
{% endfor %}

**Note:** due to the pace of new model releases, this section is doomed to be out of date.

## üìö Datasets

Datasets relevant for training models.

| Name                            | Size       | Languages   | :star:  | Released   | Notes   |
| ------------------------------- | ---------- | ----------- | ------- | ---------- | ------- |
{% for item in datasets %}| [{{ item.name }}]({{ item.link }}) | {{ item.size }} | {{ item.languages }} | {{ item.stars }} | {{ item.released }} | {{ item.notes }} |
{% endfor %}

## Tools

Misc relevant useful tools.

| Name                            | :star:  | Released   | Notes   |
| ------------------------------- | ------- | ---------- | ------- |
{% for item in misc %}| [{{ item.name }}]({{ item.link }}) | {{ item.stars }} | {{ item.released }} | {{ item.notes }} |
{% endfor %}

## Suggested setup

As you can see above there are many options for models and editor extensions. If you use VS Code or JetBrains and want to get started straight away you can use the following setup:

1. Install [LM Studio](https://lmstudio.ai/).
2. Install [Continue.dev](https://www.continue.dev/) extension.
3. Download one or several models in LM Studio. As of January 2025, Qwen 2.5 Coder is a good choice for autocomplete and Deepseek R1 is a good choice for chat. Depending on your hardware you'll have to experiment with which model size and quantization level gives you sufficient speed. For example on a Macbook Pro M2 with 32GB RAM, `Qwen2.5-Coder-7B-Instruct-Q4_K_M` works well for autocomplete and `DeepSeek-R1-Distill-Qwen-14B-Q4_0` works well for chat.
4. Go to the Developer tab in LM Studio and start the server.
5. Configure Continue.dev extension with by adding your selected models. For example:
    ```json
    {
        "models": [
            {
            "apiBase": "http://localhost:1234/v1/",
            "title": "Deepseek R1",
            "model": "bartowski/deepseek-r1-distill-qwen-14b",
            "provider": "lmstudio"
            }
        ],
        "tabAutocompleteModel": {
            "provider": "lmstudio",
            "apiBase": "http://localhost:1234/v1/",
            "title": "Qwen 2.5 Coder",
            "model": "qwen2.5-coder-7b-instruct"
        },
    }

## üì∞ History

- üê¶ [Tweet announcing this repo][announce]

[announce]: https://twitter.com/ErikBjare/status/1681616666600394753

## üìà Stats

Stargazers over time:

[![Stargazers over time](https://starchart.cc/ErikBjare/are-copilots-local-yet.svg)](https://starchart.cc/ErikBjare/are-copilots-local-yet)
